{"cells":[{"metadata":{"_uuid":"674378297bf7f78937a4f1aa466932e68466efcd"},"cell_type":"markdown","source":"### Data Pre-processing Stage\n\n  This notebook contains the basic data pre processing steps.\n  * Preprocessing refers to the transformations applied to the data before feeding it to the machine learning algorithms.\n  * The data gathered from different sources is collected in raw format which is not feasible for the analysis.\n  * Data Preprocessing technique is used to convert the raw data into a clean data set."},{"metadata":{"_uuid":"fe10f16ae89b1e2579469abba5542de59d18ca7d"},"cell_type":"markdown","source":"#### Why preprocessing ?\n1. Real world data are generally\n    * Incomplete: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data.\n    * Noisy: containing errors or outliers.\n    * Inconsistent: containing discrepancies in codes or names."},{"metadata":{"_uuid":"a30f7f7e2a855e2f0ef0bfc096bd4cc61a3399d9"},"cell_type":"markdown","source":"Let's take a sample dataset for this exercise.\nThis dataset named \"data.csv\" contains whether a user purchased the product or not.\nThe users data has age,salary and the country they belonged to."},{"metadata":{"trusted":true,"_uuid":"3ed37978d39bc4b4497ed16fd66a99a40df07561"},"cell_type":"code","source":"###############################################################\n#       Step 1 : Importing the libraries                      #\n###############################################################\n\n\n# NumPy is module for Python. The name is an acronym for \"Numeric Python\" or \"Numerical Python\".\n# This makes sure that the precompiled mathematical and numerical functions \n# and functionalities of Numpy guarantee great execution speed.\n\nimport numpy as np\n\n# Pandas is an open-source Python Library providing high-performance data manipulation \n# and analysis tool using its powerful data structures. \n# The name Pandas is derived from the word Panel Data – an Econometrics from Multidimensional data.\n\nimport pandas as pd\n\n\n# The OS module in Python provides a way of using operating system dependent functionality. \n# The functions that the OS module provides allows you to interface with the underlying operating system \n# that Python is running on – be that Windows, Mac or Linux.\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d09edc61b5ef258b23b6de01c4745613a4d00eca"},"cell_type":"code","source":"###############################################################\n#       Step 2 : Importing the Dataset                        #\n###############################################################\n\n#Read the 'Data.csv' and store the data in the vairable dataset.\ndataset = pd.read_csv(\"../input/Data.csv\")\nprint('Load the datasets...')\n\n\n# Print the shape of the dataset\nprint ('dataset: %s'%(str(dataset.shape)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2255fd0287097870845b0b4dbfd4e4ac04ea9f9"},"cell_type":"markdown","source":"The dataset contains 15 rows and 4 columns"},{"metadata":{"trusted":true,"_uuid":"2764bded9c4328f4ca9902a333d882b12d09223e"},"cell_type":"code","source":"# print the dataset\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b51a61e0f0b243b1b3c5650a30dcf02d6f943a5"},"cell_type":"code","source":"# Separate the dependent and independent variables\n\n# Independent variable\n# iloc[rows,columns]\n# Take all rows\n# Take last but one column from the dataset (:-1)\nX = dataset.iloc[:,:-1].values\n\n# Dependent variable\n# iloc[rows,columns]\n# Take all rows\n# Take last column from the dataset (:-1)\nY = dataset.iloc[:,3].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5351fbd6905b93aad9c455ee99cb0814e55cfc58"},"cell_type":"code","source":"# Print the X and Y\nprint ('X: %s'%(str(X)))\nprint ('-----------------------------------')\nprint ('Y: %s'%(str(Y)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dffee0eb70165f04561442a9f05bf2fd0c0d205f"},"cell_type":"markdown","source":"#### 1. Handle Missing Data\n\nThere are few missing data in the Age and salary columns (NaN values).\n\n#### i. Deleting Rows:\n*      We cannot remove the rows with the missing data as it will affect the output of the  machine learning algorithm.\n*      However we can delete a particular row if it has a null value for a particular feature and a particular column if it has more than 70-75% of missing values.\n      \n\n#### ii. Replacing With Mean/Median/Mode:\n*      This strategy can be applied on a feature which has numeric data like the age of a person.\n*      We can calculate the mean, median or mode of the feature and replace it with the missing values.    \n*     The loss of the data can be negated by this method which yields better results compared to removal of rows and  \n*       columns.\n*      Replacing with the above three approximations are a statistical approach of handling the missing values. \n*     This method is also called as leaking the data while training. \n*     Another way is to approximate it with the deviation of neighbouring values. \n*     This works better if the data is linear.\n"},{"metadata":{"trusted":true,"_uuid":"bc0f9fc1fde9aff85963ee3db02f1fd943d01d93"},"cell_type":"code","source":"###############################################################\n#       Step 3 : Missing Data                                 #\n###############################################################\n\n# Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python.\n# The sklearn.preprocessing package provides several common utility functions and transformer classes \n# to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n\nfrom sklearn.preprocessing import Imputer\n\n# Imputer Class takes the follwing parameters:\n#     missing_values : The missing values in our dataset are called as NaN (Not a number).Default is NaN\n#     strategy       : replace the missing values by mean/median/mode. Default is mean.\n#     axis           : if axis = 0, we take we of the column and if axis = 1, we take mean value of row.\n\nimputer = Imputer(missing_values = 'NaN',strategy = 'mean', axis = 0)\n\n# Fit the imputer on X.\n# Take all rows and columns only with the missing values.\n# Note: Index starts with 0. Upper bound (3) is not included.\n\n# Fit imputer for columns 1 and 2 of X matrix.\nimputer = imputer.fit(X[:,1:3])\n\n#Replace missing data with mean of column\nX[:,1:3] = imputer.transform(X[:,1:3])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbb90e1ca86164287023c7fc38197bf087e0ef0e"},"cell_type":"code","source":"print ('X: %s'%(str(X)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e06044bf541de146377c72fe4f34eb83100c2f49"},"cell_type":"markdown","source":"* Mean Value of Age    = Sum of all age values /14   = 33.714285714285715\n* Mean Value of Salary = Sum of all Salary value /14 = 54857.142857142855"},{"metadata":{"_uuid":"0504a7e75bde4ac7779062a37f144d9b9a096a17"},"cell_type":"markdown","source":"#### 2. Encode the Categorical data\n\nCategorical data are variables that contain label values rather than numeric values.\nSome algorithms can work with categorical data directly.\n\nFor example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation).\n\nMany machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n\nThis means that categorical data must be converted to a numerical form."},{"metadata":{"_uuid":"a8261920df0d266e0551cdae8264a07c96cdd7f8"},"cell_type":"markdown","source":"In our dataset there are 2 columns with categorical data.\n\nThe First column which contains the country and the last column purchased."},{"metadata":{"_uuid":"28a61d344d1556bdac352ac69e2ed0eb0e9a0cbc"},"cell_type":"markdown","source":"#### i.  Label Encoder: \n\n    * It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).\n    * Numerical labels are always between 0 and n_classes-1.     \n\n#### ii. OneHotEncoder:\n    * Encode categorical integer features using a one-hot aka one-of-K scheme.\n    * The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) \n      features.\n    * The output will be a sparse matrix where each column corresponds to one possible value of one feature.\n    * It is assumed that input features take on values in the range [0, n_values]\n    * This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs\n      with the standard kernels.        "},{"metadata":{"trusted":true,"_uuid":"ad860887b31ba69dfe706ccd320b9cebe4fe931b"},"cell_type":"code","source":"###############################################################\n#       Step 4 : Categorical variables                        #\n###############################################################\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\n\nlabelencoder_X = LabelEncoder()\nX[:,0] = labelencoder_X.fit_transform(X[:,0])\nX[:,0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f745a4a68c5e1d90dc1537940af301e69d9c164"},"cell_type":"markdown","source":"Now the categorical data of the country value is changed to numerical value.\n\n| Country | Value |\n|:--------|:------|\n| China   |   0   |  \n| India   |   1   |   \n| Srilanka|   2   |   \n"},{"metadata":{"_uuid":"9c4a6e53a3a028c11f1d9a91b9f45fbf8c64c996"},"cell_type":"markdown","source":"#### Dummy Encoding\n\n    * The above encoding will result in a problem.\n    * The label encoding transforms the data as shown in the table above.\n    * The Machine learning algorithm will assume that China>India>Sri Lanka.\n    * But this is not the case. We just converted the categorical value and assigned it to a numeric value.\n    * Hence there is a need to apply Dummy encoding to the above dataset.\n\n| Country | China | India | Sri Lanka |\n|:--------|:------|:------|:----------|\n| China   |   1   |  0    |    0      |   \n| India   |   0   |  1    |    0      |   \n| Srilanka|   0   |  0    |    1      |   \n| India   |   0   |  1    |    0      |  \n| Srilanka|   0   |  0    |    1      |  \n| China   |   1   |  0    |    0      |  \n  \n\n"},{"metadata":{"trusted":true,"_uuid":"2594e6f27a4b8a3fda17c7def917c1884546bae1"},"cell_type":"code","source":"# Applying the OneHotEncoder to the first column[0]\nonhotencoder = OneHotEncoder(categorical_features = [0])\nX=onhotencoder.fit_transform(X).toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"614c26108b7dcd141f65bf14b67e178aee962f19"},"cell_type":"code","source":"# Encoding the categorical data for Y matrix\nlabelencoder_Y = LabelEncoder()\nY = labelencoder_X.fit_transform(Y)\nY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19b3d19ab1e4b1c1ff425d55b0eb79317bf7ab86"},"cell_type":"code","source":"###############################################################\n#       Step 5 : Splitting the dataset                        #\n###############################################################\n\nfrom sklearn.cross_validation import train_test_split\n\n# The test size is taken as 20% of the total dataset i.e., out of 15 only 3 rows are taken as test set\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"780861983da7a12d75a1a3c7e6e03a1602d41c07","scrolled":true},"cell_type":"code","source":"# Print the shape of the dataset\nprint ('X_train: %s'%(str(X_train.shape)))\nprint ('----------------')\nprint ('X_test: %s'%(str(X_test.shape)))\nprint ('----------------')\nprint ('Y_train: %s'%(str(Y_train.shape)))\nprint ('----------------')\nprint ('Y_test: %s'%(str(Y_test.shape)))\nprint ('----------------')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ef42f4efa7b7ed1f21966153a9a0de7a14d1d2e"},"cell_type":"markdown","source":"#### 3. Scale your Features\n    *  Most of the times, the dataset will contain features highly varying in magnitudes, units and range.\n    *  Since the machine learning algorithms use Eucledian distance between two data points in their computations, this is\n       result in wrong prediction.\n      \nWe need to put the variables in same range, in the same scale so that no variable dominates the other variable.     \n      \n      "},{"metadata":{"trusted":true,"_uuid":"d7b0722d744c2da299c42cc2b6e50f6a21a87d20"},"cell_type":"code","source":"###############################################################\n#       Step 6 : Feature Scaling                              #\n###############################################################\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_X = StandardScaler()\n\n# We need to fit and transform the training set\nX_train = sc_X.fit_transform(X_train)\n\n# We need to fit the test set\nX_test = sc_X.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19edd09629c93a036a69dbb5adc2fa526c1403db"},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"047b9e920be562717de782098c416e0bcc6e9bd5"},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d884daf43f80af5c65844fbc763ea18c738890ec"},"cell_type":"markdown","source":"Now the all the data are in same scale. We can now apply different Machine learning model to the dataset."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}